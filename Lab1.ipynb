{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Lab1.ipynb","version":"0.3.2","provenance":[{"file_id":"190wC9_DZs7JFYaV8PumXsKgkrp_cBBaI","timestamp":1555097321664},{"file_id":"1bk4sbmYjGNA3e4r2e57xgvZI5t316XBm","timestamp":1554062829584},{"file_id":"https://github.com/margaretmz/deep-learning/blob/master/fashion_mnist_keras.ipynb","timestamp":1554051019235}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WCzyee4vgEx4","colab_type":"text"},"cell_type":"markdown","source":["# **Laboration 1 - CNN**\n","\n","The task is  to encode a convolutional neural network that analyzes Fashion-Mnist data which is a dataset of Zalando’s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28×28 grayscale image, associated with a label from 10 classes. At least two different settings on one of the parameters should be run and compared and final results commented. Accuracy should reach at least 90% right on test data in the case of Fashion-Minst.\n","\n","The program written to solve the task is divided into a function that processes data, creates and evaluates the model.The created model arcitecture is a simple network consisting of two 2D Convolutional layers (Conv2D). \n","\n","The model start by using smaller filters to collect as much local information as possible, and then gradually increase the filter width to reduce the generated feature space width to represent more global, high-level and representative information. \n","\n","The number of epocs is set to 50. As the training proceeds, the fit method outputs information showing the progress of each epoch, how long the epoch took to execute and the evalutation metics for the pass. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch.  The lower the loss, the better the neural network is at predicting what each image is. The \"accuracy\" metric is used to see the accuracy score when training the model.  \n","\n","The parameter chosen to change the setting on is padding, which can take on one of two values: \"valid\" or \"same\".  \"Valid\" means no padding only valid window locations will be used. \"Same\" results in padding the input such that the output has the same length as the original input. The padding argument argument defaults to valid. In both cases it's easy to reach an accuracy over 90% but around 92-93% it stucks very fast. The difference in performance is not that great. But there are couple of reasons why using padding is important. It's easier to design networks if the height and width is preserved.  We don't need to worry about tensor dimensions when going from one layer to another which allows us to design deeper networks. Without padding, reduction in volume size would reduce too quickly. Padding improves performance by keeping information at the borders.\n","\n","\n","**References**\n","\n","Chollet, F., (2018).* Deep Learning with Python*, Manning, Shelter Island.\n","\n","https://medium.com/tensorflow/hello-deep-learning-fashion-mnist-with-keras-50fcff8cd74a\n","\n","https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n","\n","https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-ii-hyper-parameter-42efca01e5d7"]},{"metadata":{"id":"QgTZ47SsZZg4","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_model(PADDING, EPOCHS):\n","  \n","  # Load the fashion-mnist train data and test data.\n","\n","  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","  \n","  # Normalize and scale data to the range of [0,1]. \n","  \n","  x_train = x_train.astype('float32') / 255\n","  x_test = x_test.astype('float32') / 255\n","  \n","  # Break training data into train/validation sets. \n","  \n","  (x_train, x_valid) = x_train[5000:], x_train[:5000] \n","  (y_train, y_valid) = y_train[5000:], y_train[:5000]\n","\n","  # Reshape input data from (28, 28) to (28, 28, 1).\n","  \n","  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","  x_valid = x_valid.reshape(x_valid.shape[0], 28, 28, 1)\n","  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","\n","  # One-hot encode the labels. \n","  \n","  y_train = tf.keras.utils.to_categorical(y_train, 10)\n","  y_valid = tf.keras.utils.to_categorical(y_valid, 10)\n","  y_test = tf.keras.utils.to_categorical(y_test, 10)\n","  \n","  # Define model architecture. The first line declares the model type as Sequential() which allows stacking sequential layers of the network.  \n","  \n","  model = tf.keras.Sequential()\n","   \n","  # The \"add()\" function is used to add layers to the model. \n"," \n","  model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=PADDING, activation='relu', input_shape=(28,28,1))) \n","  \n","  # MaxPooling is used to reduce the spatial dimensions of the output volume. \n","  \n","  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n","  \n","  # Dropout is a regularization technique for reducing overfitting.\n","  \n","  model.add(tf.keras.layers.Dropout(0.3))  \n","  \n","  model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1),padding=PADDING, activation='relu'))\n","  model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n","  model.add(tf.keras.layers.Dropout(0.3))\n"," \n","  # Flattening is needed to flaten the previous layers 3D output before passing the to the Dense layer.\n","  \n","  model.add(tf.keras.layers.Flatten())  \n","  \n","  # For Dense layers, the first parameter is the output size of the layer. \n","  \n","  model.add(tf.keras.layers.Dense(256, activation='relu'))\n","  model.add(tf.keras.layers.Dropout(0.5))\n","  \n","  # The final layer has an output size of 10, corresponding to the 10 classes of digits.\n","  \n","  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","  \n","  # Compiling the model takes three parameters: loss, optimizer, and metrics. \n","  \n","  model.compile(loss='categorical_crossentropy',\n","             optimizer='adam',\n","             metrics=['accuracy'])\n","  \n","  # Fit the model which is the stage where the forward and backward propagation occur.\n","  \n","  model.fit(x_train,\n","         y_train,\n","         batch_size=64,\n","         epochs=EPOCHS,\n","         validation_data=(x_valid, y_valid))\n","  \n","  # Evaluate the model on test set.\n","  \n","  loss, accuracy = model.evaluate(x_test, y_test, verbose=1) # verbose=1 outputs a progressbar that shows the status of the current epoch.\n","\n","  # Print test accuracy and loss.\n","  \n","  print(\"\\n\", \"Accuray:\", accuracy, \"Loss:\", loss) "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_3NZFQx8ZAQf","colab_type":"code","outputId":"55894fb1-f3bc-4682-d11b-7dcc3d5b0671","executionInfo":{"status":"ok","timestamp":1555099682709,"user_tz":-120,"elapsed":1070367,"user":{"displayName":"Patrik Andersson","photoUrl":"","userId":"06619808885335312072"}},"colab":{"base_uri":"https://localhost:8080/","height":3726}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","# The number of epochs is the number of times the model will cycle through the data.\n","\n","EPOCHS=50\n","\n","# The same parameter means that the output volume size maches the input volume size.\n","\n","PADDING='valid'\n","\n","create_model(PADDING, EPOCHS)\n","\n","# With the valid parameter the input volume is not zero-padded and the spatial dimensions are allowed to be reduced.\n","\n","PADDING='same'\n","\n","create_model(PADDING, EPOCHS)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Train on 55000 samples, validate on 5000 samples\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Epoch 1/50\n","55000/55000 [==============================] - 12s 209us/sample - loss: 0.6338 - acc: 0.7675 - val_loss: 0.3839 - val_acc: 0.8636\n","Epoch 2/50\n","55000/55000 [==============================] - 10s 185us/sample - loss: 0.4211 - acc: 0.8478 - val_loss: 0.3210 - val_acc: 0.8874\n","Epoch 3/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.3675 - acc: 0.8664 - val_loss: 0.2793 - val_acc: 0.8994\n","Epoch 4/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.3396 - acc: 0.8760 - val_loss: 0.2643 - val_acc: 0.9044\n","Epoch 5/50\n","55000/55000 [==============================] - 10s 179us/sample - loss: 0.3188 - acc: 0.8829 - val_loss: 0.2566 - val_acc: 0.9078\n","Epoch 6/50\n","55000/55000 [==============================] - 11s 193us/sample - loss: 0.3022 - acc: 0.8890 - val_loss: 0.2466 - val_acc: 0.9102\n","Epoch 7/50\n","55000/55000 [==============================] - 11s 202us/sample - loss: 0.2893 - acc: 0.8920 - val_loss: 0.2311 - val_acc: 0.9144\n","Epoch 8/50\n","55000/55000 [==============================] - 10s 186us/sample - loss: 0.2794 - acc: 0.8956 - val_loss: 0.2352 - val_acc: 0.9158\n","Epoch 9/50\n","55000/55000 [==============================] - 10s 181us/sample - loss: 0.2725 - acc: 0.8973 - val_loss: 0.2191 - val_acc: 0.9184\n","Epoch 10/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.2676 - acc: 0.9007 - val_loss: 0.2280 - val_acc: 0.9128\n","Epoch 11/50\n","55000/55000 [==============================] - 10s 181us/sample - loss: 0.2617 - acc: 0.9026 - val_loss: 0.2172 - val_acc: 0.9190\n","Epoch 12/50\n","55000/55000 [==============================] - 10s 183us/sample - loss: 0.2545 - acc: 0.9043 - val_loss: 0.2157 - val_acc: 0.9180\n","Epoch 13/50\n","55000/55000 [==============================] - 10s 181us/sample - loss: 0.2515 - acc: 0.9064 - val_loss: 0.2114 - val_acc: 0.9208\n","Epoch 14/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.2449 - acc: 0.9071 - val_loss: 0.2261 - val_acc: 0.9190\n","Epoch 15/50\n","55000/55000 [==============================] - 10s 183us/sample - loss: 0.2408 - acc: 0.9093 - val_loss: 0.2258 - val_acc: 0.9178\n","Epoch 16/50\n","55000/55000 [==============================] - 10s 181us/sample - loss: 0.2392 - acc: 0.9114 - val_loss: 0.2031 - val_acc: 0.9258\n","Epoch 17/50\n","55000/55000 [==============================] - 10s 183us/sample - loss: 0.2351 - acc: 0.9110 - val_loss: 0.2055 - val_acc: 0.9234\n","Epoch 18/50\n","55000/55000 [==============================] - 10s 183us/sample - loss: 0.2315 - acc: 0.9137 - val_loss: 0.2099 - val_acc: 0.9200\n","Epoch 19/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.2254 - acc: 0.9155 - val_loss: 0.2052 - val_acc: 0.9234\n","Epoch 20/50\n","55000/55000 [==============================] - 8s 153us/sample - loss: 0.2268 - acc: 0.9147 - val_loss: 0.2255 - val_acc: 0.9182\n","Epoch 21/50\n","55000/55000 [==============================] - 8s 144us/sample - loss: 0.2227 - acc: 0.9166 - val_loss: 0.2065 - val_acc: 0.9242\n","Epoch 22/50\n","55000/55000 [==============================] - 8s 144us/sample - loss: 0.2173 - acc: 0.9166 - val_loss: 0.2108 - val_acc: 0.9188\n","Epoch 23/50\n","55000/55000 [==============================] - 8s 143us/sample - loss: 0.2154 - acc: 0.9182 - val_loss: 0.2131 - val_acc: 0.9188\n","Epoch 24/50\n","55000/55000 [==============================] - 8s 142us/sample - loss: 0.2169 - acc: 0.9201 - val_loss: 0.2037 - val_acc: 0.9230\n","Epoch 25/50\n","55000/55000 [==============================] - 8s 143us/sample - loss: 0.2126 - acc: 0.9193 - val_loss: 0.2015 - val_acc: 0.9248\n","Epoch 26/50\n","55000/55000 [==============================] - 8s 144us/sample - loss: 0.2090 - acc: 0.9205 - val_loss: 0.1987 - val_acc: 0.9278\n","Epoch 27/50\n","55000/55000 [==============================] - 8s 144us/sample - loss: 0.2072 - acc: 0.9225 - val_loss: 0.1993 - val_acc: 0.9244\n","Epoch 28/50\n","55000/55000 [==============================] - 8s 143us/sample - loss: 0.2077 - acc: 0.9212 - val_loss: 0.1952 - val_acc: 0.9258\n","Epoch 29/50\n","55000/55000 [==============================] - 8s 149us/sample - loss: 0.2050 - acc: 0.9222 - val_loss: 0.1970 - val_acc: 0.9298\n","Epoch 30/50\n","55000/55000 [==============================] - 8s 148us/sample - loss: 0.2056 - acc: 0.9222 - val_loss: 0.2036 - val_acc: 0.9256\n","Epoch 31/50\n","55000/55000 [==============================] - 8s 150us/sample - loss: 0.2006 - acc: 0.9237 - val_loss: 0.2033 - val_acc: 0.9248\n","Epoch 32/50\n","55000/55000 [==============================] - 8s 153us/sample - loss: 0.1986 - acc: 0.9242 - val_loss: 0.2015 - val_acc: 0.9264\n","Epoch 33/50\n","55000/55000 [==============================] - 9s 156us/sample - loss: 0.1968 - acc: 0.9247 - val_loss: 0.1993 - val_acc: 0.9276\n","Epoch 34/50\n","55000/55000 [==============================] - 8s 151us/sample - loss: 0.1993 - acc: 0.9248 - val_loss: 0.1975 - val_acc: 0.9284\n","Epoch 35/50\n","55000/55000 [==============================] - 9s 162us/sample - loss: 0.1993 - acc: 0.9233 - val_loss: 0.1975 - val_acc: 0.9228\n","Epoch 36/50\n","55000/55000 [==============================] - 9s 159us/sample - loss: 0.1925 - acc: 0.9267 - val_loss: 0.2037 - val_acc: 0.9244\n","Epoch 37/50\n","55000/55000 [==============================] - 9s 162us/sample - loss: 0.1952 - acc: 0.9254 - val_loss: 0.2039 - val_acc: 0.9238\n","Epoch 38/50\n","55000/55000 [==============================] - 10s 184us/sample - loss: 0.1947 - acc: 0.9262 - val_loss: 0.2012 - val_acc: 0.9264\n","Epoch 39/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.1887 - acc: 0.9287 - val_loss: 0.2041 - val_acc: 0.9252\n","Epoch 40/50\n","55000/55000 [==============================] - 10s 186us/sample - loss: 0.1947 - acc: 0.9269 - val_loss: 0.1987 - val_acc: 0.9276\n","Epoch 41/50\n","55000/55000 [==============================] - 10s 186us/sample - loss: 0.1886 - acc: 0.9284 - val_loss: 0.2072 - val_acc: 0.9272\n","Epoch 42/50\n","55000/55000 [==============================] - 10s 183us/sample - loss: 0.1903 - acc: 0.9280 - val_loss: 0.1952 - val_acc: 0.9298\n","Epoch 43/50\n","55000/55000 [==============================] - 10s 182us/sample - loss: 0.1872 - acc: 0.9293 - val_loss: 0.1992 - val_acc: 0.9262\n","Epoch 44/50\n","55000/55000 [==============================] - 10s 179us/sample - loss: 0.1888 - acc: 0.9277 - val_loss: 0.1998 - val_acc: 0.9258\n","Epoch 45/50\n","55000/55000 [==============================] - 10s 177us/sample - loss: 0.1873 - acc: 0.9281 - val_loss: 0.2048 - val_acc: 0.9266\n","Epoch 46/50\n","55000/55000 [==============================] - 10s 183us/sample - loss: 0.1873 - acc: 0.9291 - val_loss: 0.2035 - val_acc: 0.9272\n","Epoch 47/50\n","55000/55000 [==============================] - 10s 187us/sample - loss: 0.1870 - acc: 0.9278 - val_loss: 0.1960 - val_acc: 0.9288\n","Epoch 48/50\n","55000/55000 [==============================] - 10s 185us/sample - loss: 0.1814 - acc: 0.9305 - val_loss: 0.1955 - val_acc: 0.9274\n","Epoch 49/50\n","55000/55000 [==============================] - 10s 187us/sample - loss: 0.1846 - acc: 0.9295 - val_loss: 0.2123 - val_acc: 0.9240\n","Epoch 50/50\n","55000/55000 [==============================] - 10s 188us/sample - loss: 0.1836 - acc: 0.9309 - val_loss: 0.1972 - val_acc: 0.9298\n","10000/10000 [==============================] - 1s 117us/sample - loss: 0.2277 - acc: 0.9199\n","\n"," Accuray: 0.9199 Loss: 0.22773224944472312\n","Train on 55000 samples, validate on 5000 samples\n","Epoch 1/50\n","55000/55000 [==============================] - 12s 216us/sample - loss: 0.5592 - acc: 0.7971 - val_loss: 0.3565 - val_acc: 0.8760\n","Epoch 2/50\n","55000/55000 [==============================] - 11s 209us/sample - loss: 0.3753 - acc: 0.8655 - val_loss: 0.2896 - val_acc: 0.8928\n","Epoch 3/50\n","55000/55000 [==============================] - 12s 209us/sample - loss: 0.3264 - acc: 0.8810 - val_loss: 0.2686 - val_acc: 0.8990\n","Epoch 4/50\n","55000/55000 [==============================] - 11s 208us/sample - loss: 0.2979 - acc: 0.8906 - val_loss: 0.2597 - val_acc: 0.9062\n","Epoch 5/50\n","55000/55000 [==============================] - 11s 207us/sample - loss: 0.2808 - acc: 0.8975 - val_loss: 0.2319 - val_acc: 0.9130\n","Epoch 6/50\n","55000/55000 [==============================] - 11s 208us/sample - loss: 0.2676 - acc: 0.9014 - val_loss: 0.2174 - val_acc: 0.9190\n","Epoch 7/50\n","55000/55000 [==============================] - 11s 207us/sample - loss: 0.2542 - acc: 0.9062 - val_loss: 0.2140 - val_acc: 0.9212\n","Epoch 8/50\n","55000/55000 [==============================] - 11s 206us/sample - loss: 0.2451 - acc: 0.9094 - val_loss: 0.2254 - val_acc: 0.9116\n","Epoch 9/50\n","55000/55000 [==============================] - 11s 208us/sample - loss: 0.2369 - acc: 0.9110 - val_loss: 0.2019 - val_acc: 0.9302\n","Epoch 10/50\n","55000/55000 [==============================] - 11s 204us/sample - loss: 0.2271 - acc: 0.9150 - val_loss: 0.2013 - val_acc: 0.9262\n","Epoch 11/50\n","55000/55000 [==============================] - 11s 206us/sample - loss: 0.2200 - acc: 0.9166 - val_loss: 0.1980 - val_acc: 0.9286\n","Epoch 12/50\n","55000/55000 [==============================] - 11s 207us/sample - loss: 0.2173 - acc: 0.9183 - val_loss: 0.1982 - val_acc: 0.9264\n","Epoch 13/50\n","55000/55000 [==============================] - 11s 207us/sample - loss: 0.2101 - acc: 0.9210 - val_loss: 0.1887 - val_acc: 0.9320\n","Epoch 14/50\n","55000/55000 [==============================] - 11s 209us/sample - loss: 0.2039 - acc: 0.9236 - val_loss: 0.2003 - val_acc: 0.9238\n","Epoch 15/50\n","55000/55000 [==============================] - 11s 205us/sample - loss: 0.1998 - acc: 0.9246 - val_loss: 0.1917 - val_acc: 0.9296\n","Epoch 16/50\n","55000/55000 [==============================] - 11s 203us/sample - loss: 0.1962 - acc: 0.9260 - val_loss: 0.1926 - val_acc: 0.9294\n","Epoch 17/50\n","55000/55000 [==============================] - 11s 205us/sample - loss: 0.1931 - acc: 0.9266 - val_loss: 0.1937 - val_acc: 0.9314\n","Epoch 18/50\n","55000/55000 [==============================] - 11s 201us/sample - loss: 0.1894 - acc: 0.9288 - val_loss: 0.1881 - val_acc: 0.9310\n","Epoch 19/50\n","55000/55000 [==============================] - 11s 201us/sample - loss: 0.1859 - acc: 0.9292 - val_loss: 0.1855 - val_acc: 0.9328\n","Epoch 20/50\n","55000/55000 [==============================] - 11s 201us/sample - loss: 0.1857 - acc: 0.9300 - val_loss: 0.1870 - val_acc: 0.9348\n","Epoch 21/50\n","55000/55000 [==============================] - 11s 201us/sample - loss: 0.1804 - acc: 0.9323 - val_loss: 0.1882 - val_acc: 0.9330\n","Epoch 22/50\n","55000/55000 [==============================] - 11s 204us/sample - loss: 0.1788 - acc: 0.9321 - val_loss: 0.1848 - val_acc: 0.9330\n","Epoch 23/50\n","55000/55000 [==============================] - 11s 203us/sample - loss: 0.1727 - acc: 0.9337 - val_loss: 0.1874 - val_acc: 0.9336\n","Epoch 24/50\n","55000/55000 [==============================] - 11s 204us/sample - loss: 0.1725 - acc: 0.9344 - val_loss: 0.1876 - val_acc: 0.9306\n","Epoch 25/50\n","55000/55000 [==============================] - 11s 202us/sample - loss: 0.1689 - acc: 0.9356 - val_loss: 0.1970 - val_acc: 0.9312\n","Epoch 26/50\n","55000/55000 [==============================] - 11s 203us/sample - loss: 0.1660 - acc: 0.9352 - val_loss: 0.1913 - val_acc: 0.9322\n","Epoch 27/50\n","55000/55000 [==============================] - 11s 202us/sample - loss: 0.1635 - acc: 0.9381 - val_loss: 0.1854 - val_acc: 0.9348\n","Epoch 28/50\n","55000/55000 [==============================] - 11s 204us/sample - loss: 0.1648 - acc: 0.9375 - val_loss: 0.1916 - val_acc: 0.9328\n","Epoch 29/50\n","55000/55000 [==============================] - 11s 204us/sample - loss: 0.1616 - acc: 0.9383 - val_loss: 0.1867 - val_acc: 0.9342\n","Epoch 30/50\n","55000/55000 [==============================] - 11s 203us/sample - loss: 0.1628 - acc: 0.9368 - val_loss: 0.1936 - val_acc: 0.9320\n","Epoch 31/50\n","55000/55000 [==============================] - 11s 200us/sample - loss: 0.1588 - acc: 0.9397 - val_loss: 0.1887 - val_acc: 0.9322\n","Epoch 32/50\n","55000/55000 [==============================] - 12s 226us/sample - loss: 0.1581 - acc: 0.9396 - val_loss: 0.1836 - val_acc: 0.9338\n","Epoch 33/50\n","55000/55000 [==============================] - 13s 231us/sample - loss: 0.1541 - acc: 0.9418 - val_loss: 0.2015 - val_acc: 0.9284\n","Epoch 34/50\n","55000/55000 [==============================] - 13s 230us/sample - loss: 0.1564 - acc: 0.9410 - val_loss: 0.1799 - val_acc: 0.9342\n","Epoch 35/50\n","55000/55000 [==============================] - 13s 233us/sample - loss: 0.1530 - acc: 0.9414 - val_loss: 0.1893 - val_acc: 0.9340\n","Epoch 36/50\n","55000/55000 [==============================] - 13s 230us/sample - loss: 0.1533 - acc: 0.9411 - val_loss: 0.1875 - val_acc: 0.9348\n","Epoch 37/50\n","55000/55000 [==============================] - 13s 232us/sample - loss: 0.1522 - acc: 0.9421 - val_loss: 0.1904 - val_acc: 0.9330\n","Epoch 38/50\n","55000/55000 [==============================] - 13s 233us/sample - loss: 0.1456 - acc: 0.9439 - val_loss: 0.1963 - val_acc: 0.9332\n","Epoch 39/50\n","55000/55000 [==============================] - 13s 230us/sample - loss: 0.1471 - acc: 0.9434 - val_loss: 0.2016 - val_acc: 0.9312\n","Epoch 40/50\n","55000/55000 [==============================] - 12s 217us/sample - loss: 0.1452 - acc: 0.9444 - val_loss: 0.1831 - val_acc: 0.9332\n","Epoch 41/50\n","55000/55000 [==============================] - 12s 212us/sample - loss: 0.1458 - acc: 0.9441 - val_loss: 0.1969 - val_acc: 0.9332\n","Epoch 42/50\n","55000/55000 [==============================] - 12s 214us/sample - loss: 0.1465 - acc: 0.9442 - val_loss: 0.1885 - val_acc: 0.9320\n","Epoch 43/50\n","55000/55000 [==============================] - 13s 228us/sample - loss: 0.1408 - acc: 0.9457 - val_loss: 0.1916 - val_acc: 0.9384\n","Epoch 44/50\n","55000/55000 [==============================] - 13s 229us/sample - loss: 0.1407 - acc: 0.9471 - val_loss: 0.1899 - val_acc: 0.9344\n","Epoch 45/50\n","55000/55000 [==============================] - 12s 227us/sample - loss: 0.1422 - acc: 0.9465 - val_loss: 0.1840 - val_acc: 0.9346\n","Epoch 46/50\n","55000/55000 [==============================] - 13s 232us/sample - loss: 0.1403 - acc: 0.9464 - val_loss: 0.1860 - val_acc: 0.9338\n","Epoch 47/50\n","55000/55000 [==============================] - 13s 229us/sample - loss: 0.1394 - acc: 0.9455 - val_loss: 0.1914 - val_acc: 0.9346\n","Epoch 48/50\n","55000/55000 [==============================] - 13s 230us/sample - loss: 0.1398 - acc: 0.9474 - val_loss: 0.1855 - val_acc: 0.9370\n","Epoch 49/50\n","55000/55000 [==============================] - 13s 234us/sample - loss: 0.1431 - acc: 0.9458 - val_loss: 0.1922 - val_acc: 0.9350\n","Epoch 50/50\n","55000/55000 [==============================] - 13s 231us/sample - loss: 0.1366 - acc: 0.9478 - val_loss: 0.1863 - val_acc: 0.9340\n","10000/10000 [==============================] - 1s 136us/sample - loss: 0.2192 - acc: 0.9252\n","\n"," Accuray: 0.9252 Loss: 0.2192079774618149\n"],"name":"stdout"}]},{"metadata":{"id":"6b4FEqdNXTbY","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}